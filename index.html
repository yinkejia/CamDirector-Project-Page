<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="CamDirector: Towards Long-Term Coherent Video Trajectory Editing">
  <meta name="keywords" content="Video Trajectory Editing, Video Camera Control, CamDirector">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CamDirector: Towards Long-Term Coherent Video Trajectory Editing</title>
  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/CamDirector icon crop small.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>


<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="has-text-centered">
            <img src="./static/images/camdirector_dynamic_icon3.gif" alt="CamDirector animated icon" style="width:60%; height:auto;">
          </div>
          <h1 class="title is-1 publication-title">CamDirector: Towards Long-Term Coherent Video Trajectory Editing</h1>
          <div class="is-size-4 publication-authors">
              <strong>CVPR 2026</strong> 
           </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=xFAV1X8AAAAJ&hl=en">Zhihao Shi</a><sup>1*</sup>,</span>
            <span class="author-block">
              <a href="https://yinkejia.github.io/">Kejia Yin</a><sup>2*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=y3MnZUEAAAAJ&hl=en">Weilin Wan</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://ca.linkedin.com/in/yuhongze-zhou-7b3838231">Yuhongze Zhou</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=KM4V0a8AAAAJ&hl=en">Yuanhao Yu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/xinxinzuohome/">Xinxin Zuo</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=f0V2fAYAAAAJ&hl=en">Qiang Sun</a><sup>2,6</sup>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.ca/citations?user=Asz24wcAAAAJ&hl=en">Juwei Lu</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>McMaster University,</span>
            <span class="author-block"><sup>2</sup>University of Toronto,</span>
            <span class="author-block"><sup>3</sup>The University of Hong Kong,</span>
            <span class="author-block"><sup>4</sup>McGill University,</span>
            <span class="author-block"><sup>5</sup>Concordia University</span>
            <span class="author-block"><sup>6</sup>MBZUAI</span>
          </div>
          <div class="is-size-6">
            <span><sup>*</sup> Equal contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://yinkejia.github.io/CamDirector-Project-Page/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://yinkejia.github.io/CamDirector-Project-Page/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://yinkejia.github.io/CamDirector-Project-Page/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://yinkejia.github.io/CamDirector-Project-Page/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://yinkejia.github.io/CamDirector-Project-Page/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video> -->
      <div class="has-text-centered">
        <img src="./static/images/teaser v5 2x.png" style="width:100%;">
      </div>
      <h2 class="subtitle has-text-centered">
        We present a novel video trajectory editing framework capable of generating new videos along desired trajectories from the given ones to achieve aesthetically pleasing and cinematic camera movements.
      </h2>
    </div>
  </div>
</section>

<br>
  <div class="container is-max-desktop">
  <div class="columns is-vcentered is-centered has-text-centered" style="margin-bottom: 0;">
    <div class="column"><b>Source</b></div>
    <div class="column"><b>RecamMaster</b></div>
    <div class="column"><b>TrajectoryCrafter</b></div>
  </div>

  <div id="results-carousel" class="carousel results-carousel">
    <div class="item item-apple">
      <video poster="" id="apple" autoplay controls muted loop playsinline height="100%">
        <source src="./static/videos/apple.mp4" type="video/mp4">
      </video>
    </div>
    <div class="item item-block">
      <video poster="" id="block" autoplay controls muted loop playsinline height="100%">
        <source src="./static/videos/block.mp4" type="video/mp4">
      </video>
    </div>
    <div class="item item-block">
      <video poster="" id="block" autoplay controls muted loop playsinline height="100%">
        <source src="./static/videos/paper-windmill.mp4" type="video/mp4">
      </video>
    </div>
    <div class="item item-block">
      <video poster="" id="block" autoplay controls muted loop playsinline height="100%">
        <source src="./static/videos/spin.mp4" type="video/mp4">
      </video>
    </div>
    <div class="item item-block">
      <video poster="" id="block" autoplay controls muted loop playsinline height="100%">
        <source src="./static/videos/teddy.mp4" type="video/mp4">
      </video>
    </div>
  </div>

  <div class="columns is-vcentered is-centered has-text-centered" style="margin-top: 0;">
    <div class="column"><b>GT</b></div>
    <div class="column"><b>Gen3C</b></div>
    <div class="column"><b>CamDirector</b></div>
  </div>
<br>

<br>
  <div class="container is-max-desktop">
  <div class="columns is-vcentered is-centered has-text-centered" style="margin-bottom: 0;">
    <div class="column"><b>Source</b></div>
    <div class="column"><b>RecamMaster</b></div>
    <div class="column"><b>TrajectoryCrafter</b></div>
  </div>

  <div id="results-carousel" class="carousel results-carousel">
    <div class="item item-container">
      <video poster="" id="container" autoplay controls muted loop playsinline height="100%">
        <source src="./static/videos/container.mp4" type="video/mp4">
      </video>
    </div>
    <div class="item item-door2">
      <video poster="" id="door2" autoplay controls muted loop playsinline height="100%">
        <source src="./static/videos/door2_aligned.mp4" type="video/mp4">
      </video>
    </div>
    <div class="item item-exercise2">
      <video poster="" id="excercise2" autoplay controls muted loop playsinline height="100%">
        <source src="./static/videos/exercise2_aligned.mp4" type="video/mp4">
      </video>
    </div>
    <div class="item item-library">
      <video poster="" id="library" autoplay controls muted loop playsinline height="100%">
        <source src="./static/videos/library.mp4" type="video/mp4">
      </video>
    </div>
    <div class="item item-mural4">
      <video poster="" id="mural4" autoplay controls muted loop playsinline height="100%">
        <source src="./static/videos/mural4.mp4" type="video/mp4">
      </video>
    </div>
    <div class="item item-sit">
      <video poster="" id="sit" autoplay controls muted loop playsinline height="100%">
        <source src="./static/videos/sit.mp4" type="video/mp4">
      </video>
    </div>
    <div class="item item-stairs">
      <video poster="" id="stairs" autoplay controls muted loop playsinline height="100%">
        <source src="./static/videos/stairs.mp4" type="video/mp4">
      </video>
    </div>
    <div class="item item-taiji2">
      <video poster="" id="taiji2" autoplay controls muted loop playsinline height="100%">
        <source src="./static/videos/taiji2_aligned.mp4" type="video/mp4">
      </video>
    </div>
  </div>

  <div class="columns is-vcentered is-centered has-text-centered" style="margin-top: 0;">
    <div class="column"><b>GT</b></div>
    <div class="column"><b>Gen3C</b></div>
    <div class="column"><b>CamDirector</b></div>
  </div>
<br>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
         <p>
            Video (camera) trajectory editing (VTE) aims to synthesize new videos that follow user-defined camera paths while preserving scene content and plausibly inpainting previously unseen regions, upgrading amateur footage into professionally styled videos.
          </p>
          <p>
            Existing VTE methods struggle with precise camera control and long-range consistency because they either inject target poses through a limited-capacity embedding or rely on single-frame warping with only implicit cross-frame aggregation in video diffusion models.
          </p>
          <p>
            To address these issues, we introduce a new VTE framework that:
          </p>
          <ul>
            <li>
              <strong>Explicitly aggregates information</strong> across the entire source video via a hybrid warping scheme. Static regions are progressively fused into a world cache then rendered to target camera poses, while dynamic regions are directly warped; their fusion yields globally consistent coarse frames that guide refinement.
            </li>
            <li>
              <strong>Processes video segments jointly</strong> with their history via a history-guided autoregressive diffusion model, while the world cache is incrementally updated to reinforce already inpainted content, enabling long-term temporal coherence.
            </li>
          </ul>
          <p>
            Finally, we present <strong>iPhone-PTZ</strong>, a new VTE benchmark with diverse camera motions and large trajectory variations, and achieve state-of-the-art performance with fewer parameters.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method</h2>
        
        <div class="pipeline-image">
          <img src="./static/images/main figure v7.png" 
               alt="Overview of CamDirector" 
               style="width: 100%; height: auto; margin-bottom: 20px;">
        </div>
        

        <div class="content has-text-justified">
          <p>
            <strong>Overview of CamDirector.</strong> 
            <strong>Left:</strong> The hybrid warping scheme leverages the entire source video to construct coarse frames by processing dynamic and static regions separately, providing a global reference of the original scene content.
          </p>
          <p>
            <strong>Right:</strong> The CCDM conditions the generation on the coarse video via ControlNet, while source-frame tokens are concatenated with target tokens as inputs to the base T2V model to provide reliable motion and appearance priors.
          </p>
        </div>
        <div class="history-gen-image">
          <img src="./static/images/autoregressive inference v2.png" 
               alt="Illustration of history-guided autoregressive generation" 
               style="width: 100%; height: auto; margin-bottom: 20px;">
        </div>
        

        <div class="content has-text-justified">
          <p>
            <strong>History-guided autoregressive generation.</strong> 
            In each iteration, <i>T</i><sup>&star;</sup> previously generated frames serve as history to guide the synthesis of the next <i>T</i> frames, along with the corresponding <i>T</i><sup>&star;</sup> + <i>T</i> source frames as input to produce the coarse frames and provide original scene context.
          </p>
        </div>

        <div class="world-cache-image">
          <img src="./static/images/dynamic update v2.png" 
               alt="Illustration of progressive world cache update" 
               style="width: 100%; height: auto; margin-bottom: 20px;">
        </div>
        

        <div class="content has-text-justified">
          <p>
            <strong>Progressive world cache update.</strong> 
            Whenever a new segment is generated, we evenly sample <i>C</i> frames as anchors, where the newly inpainted regions are merged into the world cache. The updated regions are highlighted in red.
          </p>
        </div>
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">iPhone-PTZ Benchmark</h2>
        
        <div class="content has-text-centered">
          <div class="video-container">
            <video id="benchmark-video" autoplay controls muted loop playsinline style="width: 100%; height: auto; display: block; border-radius: 10px;">
              <source src="./static/videos/iphone_ptz_pv.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        

        <div class="content has-text-justified">
          <p> 
            <strong>iPhone-PTZ</strong> includes ten diverse scenes featuring a broad spectrum of camera motions, 
            such as dolly, pan, orbiting, and significantly larger fields of view.
          </p>
          <p>
            Each scene contains two synchronized <strong>1280 &times; 720</strong> videos, with durations ranging from 
            <strong>5-12s</strong>, captured using identical <strong>iPhone 14 Plus</strong> devices:
          </p>
          <ul>
            <li>
              <strong>Casual Setting:</strong> Recorded by casual users under handheld settings to simulate amateur footage.
            </li>
            <li>
              <strong>Professional Setting:</strong> Recorded by professional operators using a <strong>DJI Osmo Mobile 7P PTZ</strong> 
              to introduce cinematic camera motions as the ground truth for trajectory editing.
            </li>
          </ul>
        </div>
        </div>
      </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website content is licensed under a <a rel="license"
                                                href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative
            Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. If you want to reuse their <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
